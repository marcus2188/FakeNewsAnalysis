{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, we document the necessary basic exploratory analysis of the Fake and Real News Dataset provided by Kaggle. After that, we will attempt to apply various Machine Learning Models to classify our data into Real or Fake news based on their title text. The best parameters for each model is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT ALL MODULES\n",
    "##### All relevant modules used for Machine Learning analysis is imported in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE THE DATASET \n",
    "##### We obtain 2 seperate data csv files representing all real or fake data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdataobj = pd.read_csv(\"newsdataset/True.csv\")\n",
    "fakedataobj = pd.read_csv(\"newsdataset/Fake.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALISE THE DATASET\n",
    "##### We plot the dataframe of the raw data file to have a glimpse of what it looks like. Upon plot, we figure out that there are 4 columns within the dataframe. We only utilise the \"title\" column in this classification analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET LOOKS LIKE : \n",
      "                                               title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3  FBI Russia probe helped by Australian diplomat...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "2  December 31, 2017   \n",
      "3  December 30, 2017   \n"
     ]
    }
   ],
   "source": [
    "print(\"DATASET LOOKS LIKE : \")\n",
    "print(realdataobj.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The data type of the title column entries belongs to the String Class. As such, we can potentially use a vectorizer to obtain word counts and utilise word counts as input data for classification analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(realdataobj.title[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEASURE OUR RAW DATASET\n",
    "##### The length of the 2 data files amount to over 44'000 data points, which by industry standards is moderately huge. We have to consider the computational time of our models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  21417  Real News\n",
      "-  23481  Fake News\n"
     ]
    }
   ],
   "source": [
    "print(\"- \", len(realdataobj.index), \" Real News\")\n",
    "print(\"- \", len(fakedataobj.index), \" Fake News\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING\n",
    "##### We will carry out the necessary procedures in order to ensure the integrity of our data is upheld. More specifically, we will scan for any null data points with regards to the \"title\" column. No null columns are detected at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search completed\n"
     ]
    }
   ],
   "source": [
    "for columns in realdataobj.columns:\n",
    "    if pd.isnull(columns):\n",
    "        print(\"Column \", columns, \" has null values.\")\n",
    "print(\"Search completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBTAIN WORKING DATA SET WITH RELEVANT COLUMNS\n",
    "##### We will extract the \"title\" column from both real and fake data files. Next, a new column called \"Status\" will be created for each data point holding their respective classification targets such as \"Real\" or \"Fake\". We proceed by merging both data sets horizontally and creating our numpy array matrix from the values. Like every machine learning model preparation, 2 matrices containing our data and our target is derived and this will be followed by a further split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdataobj[\"Status\"] = \"Real\"\n",
    "fakedataobj[\"Status\"] = \"Fake\"\n",
    "joineddf = pd.concat([realdataobj, fakedataobj])\n",
    "joinarray = joineddf.values\n",
    "joindata = joinarray[:,0]\n",
    "jointarget = joinarray[:,-1]\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(joindata, jointarget, random_state = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VECTORISE COUNTS OF EACH WORD IN DATASET\n",
    "##### Our machine learning models will be trained based on frequency of word appearance in each news article title. Each unique word will be awarded a column, containing the number of appearances in each data point title. For instance, \"word A\" that appear 10 times in 2 seperate titles will have the value of 10 in the \"word A\" column of both of their respective rows. This creates a sparse matrix of integers, sized at about 33673 titles and 19196 unique words/columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " <33673x19196 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 409456 stored elements in Compressed Sparse Row format>\n",
      "['bharara', 'bhumibol', 'bi', 'biafra', 'bias', 'biased', 'bibi', 'bible', 'biblical', 'bicker', 'bicycle', 'bid', 'biden', 'bids', 'big', 'bigger', 'biggest', 'biggie', 'bigly', 'bigot']\n"
     ]
    }
   ],
   "source": [
    "countvec = CountVectorizer().fit(xtrain)\n",
    "xtrainvec = countvec.transform(xtrain)\n",
    "print(\"\\n\",repr(xtrainvec))\n",
    "words = countvec.get_feature_names()\n",
    "xtestvec = countvec.transform(xtest)\n",
    "print(words[2000:2020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL MACHINE LEARNING MODELS INSTANTIATED\n",
    "##### We will begin our implementation of the models by first instantiating objects of all the relevant classification algorithms. All these models are capable of using word counts to classify if a news article is likely real or fake. Some default parameters are included within instantiation while variable parameters that affect accuracy of modelling is specified in the parameter grid. We will attempt to obtain the best parameters for each algorithm using GridSearch with Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames = [\"K Nearest Neighbour\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"Kernel SVC\", \"Neural Network MLP\"]\n",
    "modellist = [KNeighborsClassifier(n_jobs = 4), LogisticRegression(max_iter = 10000, n_jobs = 4), DecisionTreeClassifier(), RandomForestClassifier(n_jobs = 4), SVC(), MLPClassifier()]\n",
    "param_gridlist = [{'n_neighbors': [3, 4, 5, 6, 7]},{'C': [0.001, 0.01, 0.1, 1, 10]}, {'max_depth': [90, 100, 110]}, {'n_estimators': [80]}, {'C': [0.001, 0.01, 0.1, 1, 10]}, {'hidden_layer_sizes': [[4], [5],[8], [10]]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEST OF K NEAREST NEIGHBOURS\n",
    "##### K nearest neighbors is a classification technique that identifies class membership based on their nearest neighboring points. The algorithm concludes when all data points are classified. Out of all the algorithms tested, K nearest neighbours provides the least accuracy in both training and test sets and the code takes 6.7 minutes to run. The optimal number of neighbours is 4, as found out by gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best K Nearest Neighbour ML algorithm best parameters : \n",
      "Best cross-validation score: 0.79\n",
      "Best parameters:  {'n_neighbors': 4}\n",
      "Train Set Accuracy : \n",
      "0.8990585929379622\n",
      "Test Set Accuracy : \n",
      "0.8038307349665924\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "grid = GridSearchCV(modellist[0], param_gridlist[0], cv=5, n_jobs = 4)\n",
    "grid.fit(xtrainvec, ytrain)\n",
    "print(\"\\nBest\", modelnames[0], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvec, ytrain))\n",
    "xtestvec = countvec.transform(xtest)\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvec, ytest))\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST OF LOGISTIC REGRESSIONS\n",
    "##### Logistic Regression is a popular classification technique that builds on Linear Regression with binary output value based on inequalities. On a binary classsification problem like ours, I expect logistic regression to perform very well in terms of accuracy of prediction. It proves to be the case as it gives an extremely high cross validation score of 0.96, and an equally impressive test set accuracy. On my run, Logistic Regression code is the fastest algorithm to execute at only 13 seconds, with C = 10 as the best regularisation parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Logistic Regression ML algorithm best parameters : \n",
      "Best cross-validation score: 0.96\n",
      "Best parameters:  {'C': 10}\n",
      "Train Set Accuracy : \n",
      "0.9993169601758085\n",
      "Test Set Accuracy : \n",
      "0.9614253897550111\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "grid = GridSearchCV(modellist[1], param_gridlist[1], cv=5, n_jobs = 4)\n",
    "grid.fit(xtrainvec, ytrain)\n",
    "print(\"\\nBest\", modelnames[1], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvec, ytrain))\n",
    "xtestvec = countvec.transform(xtest)\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvec, ytest))\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST OF DECISION TREE CLASSIFIERS\n",
    "##### Decision Trees offer comprehensive classification analysis using branch decisions and predictions to seperate data points into classes. Trees are susceptible to overfitting on the training set if the max depth of their roots is not regularised enough. Decision Trees provide disappointing cross validation marks on training, but moderate accuracy on both sets with an optimal max_depth of 100. It takes 31 seconds to run the gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Decision Tree ML algorithm best parameters : \n",
      "Best cross-validation score: 0.90\n",
      "Best parameters:  {'max_depth': 100}\n",
      "Train Set Accuracy : \n",
      "0.9735990259258159\n",
      "Test Set Accuracy : \n",
      "0.9128730512249443\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(modellist[2], param_gridlist[2], cv=5, n_jobs = 4)\n",
    "grid.fit(xtrainvec, ytrain)\n",
    "print(\"\\nBest\", modelnames[2], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvec, ytrain))\n",
    "xtestvec = countvec.transform(xtest)\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvec, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST OF RANDOM FORESTS\n",
    "##### Random Forests is basicially a conglomeration of multiple decision trees to obtain averages. This is often attributed to decision biases and randomisation that can hinder individual decision trees and their outputs. The larger the number of trees built, the longer the computation time but the more accurate the decision because of a more representative average. The highest accuracy here occurs when we allow the trees to grow infinitely with an optimal number of 80 trees in total. Random Forest here takes 2 minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Random Forest ML algorithm best parameters : \n",
      "Best cross-validation score: 0.95\n",
      "Best parameters:  {'n_estimators': 80}\n",
      "Train Set Accuracy : \n",
      "1.0\n",
      "Test Set Accuracy : \n",
      "0.9506458797327394\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(modellist[3], param_gridlist[3], cv=5, n_jobs = 4)\n",
    "grid.fit(xtrainvec, ytrain)\n",
    "print(\"\\nBest\", modelnames[3], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvec, ytrain))\n",
    "xtestvec = countvec.transform(xtest)\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvec, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST OF KERNEL SVM CLASSIFIER\n",
    "##### Kernelised Standard Vector Machines (SVMs) are a class of mathematical models that transform data inputs into centralised vectors. These vectors use magnitude(radial) and direction to determine the class of each data point. It is a high level machine learning algorithm that is known to be computationally expensive and parameter dependant, but produces excellent accuracies. For consistency across all algorithms, our data is not initially scaled down. However, this causes early data convergence for SVM. As such, 2 variants of this algorithm is ran. The non standardised dataset variant runs for a staggering 29 mins giving best parameters of C = 10 and the highest accuracy of all algorithms I've obtained despite overfitting. The standardised variant takes even longer for 17 mins to give a more unsatisfactory accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Kernel SVC ML algorithm best parameters : \n",
      "Best cross-validation score: 0.97\n",
      "Best parameters:  {'C': 10}\n",
      "Train Set Accuracy : \n",
      "1.0\n",
      "Test Set Accuracy : \n",
      "0.9692650334075724\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(modellist[4], param_gridlist[4], cv=5, n_jobs = -1)\n",
    "grid.fit(xtrainvec, ytrain)\n",
    "print(\"\\nBest\", modelnames[4], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvec, ytrain))\n",
    "xtestvec = countvec.transform(xtest)\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvec, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerobj = StandardScaler(with_mean = False)\n",
    "xtrainvecscaled = scalerobj.fit_transform(xtrainvec)\n",
    "xtestvec = countvec.transform(xtest)\n",
    "xtestvecscaled = scalerobj.fit_transform(xtestvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STANDARDISED AND SCALED :\n",
      "\n",
      "Best Kernel SVC ML algorithm best parameters : \n",
      "Best cross-validation score: 0.91\n",
      "Best parameters:  {'C': 10}\n",
      "Train Set Accuracy : \n",
      "0.9985151308169751\n",
      "Test Set Accuracy : \n",
      "0.94815144766147\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(modellist[4], param_gridlist[4], cv=5, n_jobs = -1)\n",
    "grid.fit(xtrainvecscaled, ytrain)\n",
    "print(\"STANDARDISED AND SCALED :\")\n",
    "print(\"\\nBest\", modelnames[4], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvecscaled, ytrain))\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvecscaled, ytest))\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST OF NEURAL NETWORK MLP CLASSIFIER\n",
    "##### MultiLayer Perceptron Neural Network is an artificial Neural Network consisting of many hidden layers of neurons to predict and classify data points based on an activation function specified. This is an extremely parameter dependant algorithm that can give vastly different weights to the hidden layer units and thus different results altogether. We will run 5 different variants of our MLP model with varying settings. After the execution, we found that\n",
    "##### Variant 1: Single Layer of neurons, \"relu\" activation function, \"adam\" solver, Takes 11 mins to run\n",
    "##### Variant 2: Double Layer of neurons, \"relu\" activation function, \"adam\" solver, Takes 6 mins to run\n",
    "##### Variant 3: Triple Layer of neurons, \"relu\" activation function, \"adam\" solver, Takes 5 mins to run\n",
    "##### Variant 4: 100 neurons, different alpha values, others default, Takes 1 hour to run\n",
    "##### Variant 5: 5 neurons, different activation functions and solvers, Takes 12 mins to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Neural Network MLP ML algorithm best parameters : \n",
      "Best cross-validation score: 0.96\n",
      "Best parameters:  {'hidden_layer_sizes': [5]}\n",
      "Train Set Accuracy : \n",
      "1.0\n",
      "Test Set Accuracy : \n",
      "0.955456570155902\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(modellist[5], param_gridlist[5], cv=5, n_jobs = -1)\n",
    "grid.fit(xtrainvec, ytrain)\n",
    "print(\"\\nBest\", modelnames[5], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvec, ytrain))\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvec, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Neural Network MLP ML algorithm best parameters : \n",
      "Best cross-validation score: 0.96\n",
      "Best parameters:  {'hidden_layer_sizes': [8, 8]}\n",
      "Train Set Accuracy : \n",
      "0.997505419772518\n",
      "Test Set Accuracy : \n",
      "0.9571492204899777\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(modellist[5], {'hidden_layer_sizes': [[7,7], [8,8], [9,9]]}, cv=5, n_jobs = -1)\n",
    "grid.fit(xtrainvec, ytrain)\n",
    "print(\"\\nBest\", modelnames[5], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvec, ytrain))\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvec, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Neural Network MLP ML algorithm best parameters : \n",
      "Best cross-validation score: 0.96\n",
      "Best parameters:  {'hidden_layer_sizes': [10, 10, 20]}\n",
      "Train Set Accuracy : \n",
      "1.0\n",
      "Test Set Accuracy : \n",
      "0.9576837416481069\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(modellist[5], {'hidden_layer_sizes': [[10,10,10], [10,10,20], [10,10,30]]}, cv=5, n_jobs = -1)\n",
    "grid.fit(xtrainvec, ytrain)\n",
    "print(\"\\nBest\", modelnames[5], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvec, ytrain))\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvec, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Neural Network MLP ML algorithm best parameters : \n",
      "Best cross-validation score: 0.95\n",
      "Best parameters:  {'alpha': 0.0001}\n",
      "Train Set Accuracy : \n",
      "1.0\n",
      "Test Set Accuracy : \n",
      "0.9567037861915367\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(modellist[5], {\"alpha\": [0.00001, 0.0001, 0.001]}, cv=5, n_jobs = -1)\n",
    "grid.fit(xtrainvec, ytrain)\n",
    "print(\"\\nBest\", modelnames[5], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvec, ytrain))\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvec, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Neural Network MLP ML algorithm best parameters : \n",
      "Best cross-validation score: 0.96\n",
      "Best parameters:  {'activation': 'tanh', 'hidden_layer_sizes': 5, 'solver': 'lbfgs'}\n",
      "Train Set Accuracy : \n",
      "1.0\n",
      "Test Set Accuracy : \n",
      "0.958663697104677\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(modellist[5], {'hidden_layer_sizes': [5], \"activation\": [\"relu\", \"tanh\"], \"solver\": [\"lbfgs\", \"adam\", \"sgd\"]}, cv=5, n_jobs = -1)\n",
    "grid.fit(xtrainvec, ytrain)\n",
    "print(\"\\nBest\", modelnames[5], \"ML algorithm best parameters : \")\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Train Set Accuracy : \")\n",
    "print(grid.score(xtrainvec, ytrain))\n",
    "print(\"Test Set Accuracy : \")\n",
    "print(grid.score(xtestvec, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
